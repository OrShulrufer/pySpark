{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for pyspark sql\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# imports for streaming\n",
    "from pyspark import SparkContext \n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# mllib imports \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "import time\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "ss = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"MyExamples\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Spark Context\n",
    "sc = SparkContext(\"local[2]\", \"my app\")..getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\").getOrCreate()\n",
    "\n",
    "# Opening files\n",
    "f1 = open(\"SMSSpamCollection\", \"r\")\n",
    "f2 = open(\"spam.txt\", \"w\")\n",
    "f3 = open(\"ham.txt\", \"w\")\n",
    "\n",
    "\n",
    "# parallelize input file (all emails)\n",
    "originalLines = sc.parallelize(f1)\n",
    "\n",
    "# creating temp RDDs\n",
    "tempSpam = originalLines.filter(lambda x: \"spam\" in x)\n",
    "tempHam = originalLines.filter(lambda x: \"ham\" in x)\n",
    "\n",
    "# eliminate non relevant features\n",
    "spamRdd = tempSpam.map(lambda x: x.replace(\"spam\\t\", \"\"))\n",
    "hamRdd = tempHam.map(lambda x: x.replace(\"ham\\t\", \"\"))\n",
    "\n",
    "# collecting RDDs to lists\n",
    "spam = spamRdd.collect()\n",
    "ham = hamRdd.collect()\n",
    "\n",
    "# writing lists to files\n",
    "for i in spam:\n",
    "    f2.write(i)\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "for i in ham:\n",
    "    f3.write(i)\n",
    "    f3.write(\"\\n\")\n",
    "\n",
    "# parallelize input files\n",
    "spam = sc.textFile(\"spam.txt\")\n",
    "ham = sc.textFile(\"ham.txt\")\n",
    "\n",
    "# Create a HashingTF instance to map email text to vectors of 10,000 features.\n",
    "tf = HashingTF(numFeatures=10000)\n",
    "\n",
    "# Each email is split into words, and each word is mapped to one feature.\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "hamFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))\n",
    "\n",
    "# Create LabeledPoint data sets for positive (spam) and negative (ham) examples.\n",
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = hamFeatures.map(lambda features: LabeledPoint(0, features))\n",
    "trainingData = positiveExamples.union(negativeExamples)\n",
    "\n",
    "# Cache since Logistic Regression is an iterative algorithm.\n",
    "trainingData.cache()\n",
    "\n",
    "# Run Logistic Regression using the SGD algorithm.\n",
    "model = LogisticRegressionWithLBFGS.train(trainingData)\n",
    "\n",
    "# Test on a positive example (spam) and a negative one (normal).\n",
    "posTest = tf.transform(\"Customer service annoncement.\"\n",
    "                       \" You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\".split(\" \"))\n",
    "negTest = tf.transform(\"500 New Mobiles from 2004, MUST GO! Txt: NOKIA to No:\"\n",
    "                       \" 89545 & collect yours today!From ONLY £1 www.4-tc.biz 2optout 087187262701.50gbp/mtmsg18\".split(\" \"))\n",
    "print(\"Prediction for positive test example: %g\" % model.predict(posTest))\n",
    "print(\"Prediction for negative test example: %g\" % model.predict(negTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and parallelize lines in text file\n",
    "linesRdd = sc.textFile(\"SMSSpamCollection\")\n",
    "linesRdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "\"\"\"\n",
    "Basic actions on rdd\n",
    "\"\"\"\n",
    "# show first line\n",
    "print(linesRdd.first())\n",
    "\n",
    "# filter lines\n",
    "spamRdd = linesRdd.filter(lambda line: \"spam\" in line)\n",
    "hamRdd = linesRdd.filter(lambda line: \"ham\" in line)\n",
    "\n",
    "# taking and colecting number of lines to list\n",
    "print(unitedRdd.take(2))\n",
    "\n",
    "# eliminate non relevant features\n",
    "spamRdd = spamRdd.map(lambda x: x.replace(\"spam\\t\", \"\"))\n",
    "hamRdd = hamRdd.map(lambda x: x.replace(\"ham\\t\", \"\"))\n",
    "\n",
    "#While FlatMap() is similar to Map, but FlatMap allows returning 0, 1 or more elements from map function.\n",
    "wordsRDD = hamRdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# combining words with reduce toa single variale\n",
    "combinneWords = wordsRDD.reduce(lambda x, y: x + y)\n",
    "\n",
    "# smple words whith 0.5 propobolity for every word to be samled\n",
    "wordsRDD = wordsRDD.sample(False, 0.5)\n",
    "\n",
    "# get distict words\n",
    "wordsRDD = wordsRDD.distinct()\n",
    "\n",
    "# count number of words in rdd\n",
    "wordsRDD.count()\n",
    "\n",
    "# count number of times each element acures in rdd\n",
    "wordsRDD.countByValue()\n",
    "\n",
    "# collect rdd \n",
    "words = wordsRDD.collect()\n",
    "\n",
    "\"\"\"\n",
    "Numeric RDD Operations\n",
    "count() Number of elements in the RDD\n",
    "mean() Average of the elements\n",
    "sum() Total\n",
    "max() Maximum value\n",
    "min() Minimum value\n",
    "variance() Variance of the elements\n",
    "sampleVariance() Variance of the elements, computed for a sample\n",
    "stdev() Standard deviation\n",
    "sampleStdev() Sample standard deviation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Transformations on one pair RDD ",
    " \t {(1, 2), (3, 4), (3, 6)}\n",
    "rdd.reduceByKey((x, y) => x + y)\t{(1,2), (3,10)}\n",
    "rdd.groupByKey() \t\t\t{(1, [2]), (3, [4,6])}\n",
    "rdd.mapValues(x => x+1) \t\t{(1,3), (3,5), (3,7)}\n",
    "flatMapValues(func)\n",
    "rdd.flatMapValues(x => (x to 5) {(1, 2), (1,3), (1, 4), (1, 5), (3, 4), (3,5)}\n",
    "keys() \t\t\t\t\t{1, 3, 3}\n",
    "values()\t\t\t\t\t{2, 4, 6}\n",
    "sortByKey()\t\t\t\t{(1, 2), (3, 4), (3, 6)}\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Transformations on two pair RDDs  ",
    "(rdd = {(1, 2), (3, 4), (3, 6)} other = {(3, 9)})\n",
    "rdd.subtractByKey(other) \t{(1, 2)}\n",
    "rdd.join(other) {(3, (4, 9)), (3, (6, 9))}\n",
    "rdd.rightOuterJoin(other) {(3,(Some(4),9)), (3,(Some(6),9))}\n",
    "rdd.leftOuterJoin(other) {(1,(2,None)), (3, (4,Some(9))), (3, (6,Some(9)))}\n",
    "rdd.cogroup(other) {(1,([2],[])), (3, ([4, 6],[9]))}\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Actions between two rdd's\n",
    "\"\"\"\n",
    "# union of two rdd's\n",
    "unitedRdd = spamRdd.union(hamRdd)\n",
    "\n",
    "# intersection of rdd's\n",
    "intersectRdd = spamRdd.intersection(hamRdd)\n",
    "\n",
    "# substarct rdd's\n",
    "subtractRdd = spamRdd.subtract(hamRdd)\n",
    "\n",
    "# cortasian union of rdd's\n",
    "cortasianRDD = spamRdd.cartesian(hamRdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accumulator empty line count\n",
    "\n",
    "Accumulators work as follows:\n",
    "We create them in the driver by calling the SparkContext.\n",
    "accumulator(initial Value) method, which produces an accumulator holding an initial value.\n",
    "The return type is an org.apache.spark.Accumulator[T] object, where T is the type of initialValue.\n",
    "\n",
    "\"\"\"\n",
    "# Create Accumulator[Int] initialized to 0\n",
    "blankLines = sc.accumulator(0)\n",
    "n = 0\n",
    "def extractCallSigns(line):\n",
    "    if (line.contains(\"*/a*/\")):\n",
    "        blankLines += 1\n",
    "    return line.split(\" \")\n",
    "callSigns = linesRdd.flatMap(extractCallSigns)\n",
    "print(\"Blank lines: %d\" % blankLines.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Broadcast\n",
    "\n",
    "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster\n",
    "in-order to access or use by the tasks. Instead of sending this data along with every task,\n",
    "PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce\n",
    "communication costs.\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.appName('My app').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "\n",
    "# df.show() is only for spark DataFrame\n",
    "# truncate=False will show you the full column content\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Piping to External Programs\n",
    "\n",
    "Spark provides a pipe() method on RDDs. Spark’s pipe() lets us write parts of jobs using any\n",
    "language we want as long as it can read and write to Unix standard streams.\n",
    "\"\"\"\n",
    "\n",
    "#Compute the distance of each call using an external R program\n",
    "distScript = \"./src/R/finddistance.R\"\n",
    "distScriptName = \"finddistance.R\"\n",
    "sc.addFile(distScript)\n",
    "\n",
    "pipeInputs =    #Some RDD\n",
    "distances = pipeInputs.pipe(SparkFiles.get(distScriptName))\n",
    "print distances.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplurning",
   "language": "python",
   "name": "deeplurning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
