{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import StorageLevel\n",
    "import twitter_credentials\n",
    "from textblob import TextBlob\n",
    "import re  # regex for cleaning the tweets\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"Twitter Dem\").getOrCreate()\n",
    "ssc = StreamingContext(sc, 3) # n is the batch interval in seconds\n",
    "ssc.checkpoint(\"./checkpoint\")\n",
    "sqlContext = SQLContext(sc)\n",
    "df = pd.DataFrame(columns = ['HashTag', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetAnalyzer:\n",
    "    \"\"\"\tFunctionality for analyzing and categorizing content from tweets.\"\"\"\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        tweet = re.sub(r'RT', \"\", tweet)\n",
    "        tweet = re.sub(r'@[^\\s]+', \"\", tweet)\n",
    "        tweet = re.sub(r'(https?:\\/\\/|(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,})', \"\", tweet)\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.show()\n",
    "\n",
    "tweeter_analyzer = TweetAnalyzer()\n",
    "pat = re.compile(r\"(#\\w+)\")\n",
    "\n",
    "tweets = ssc.socketTextStream(twitter_credentials.HOST, twitter_credentials.PORT, StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "tweetsWithHT = tweets.filter(lambda t: \"#\" in t)\n",
    "\n",
    "hashtags_with_sentiments = tweetsWithHT.map(lambda x: (pat.findall(x), tweeter_analyzer.analyze_sentiment(x)))\n",
    "  \n",
    "hashtags_with_sentiments_lists.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "tweetsWithHT = tweets.filter(lambda t: \"#\" in t)\n",
    "\n",
    "hashtags_with_sentiments = tweetsWithHT.map(lambda x: (pat.findall(x), tweeter_analyzer.analyze_sentiment(x)))\n",
    "hashtags_with_sentiments_lists = hashtags_with_sentiments.map(lambda elem: list(elem))\n",
    "\n",
    "hashtags_with_sentiments_lists.pprint()\n",
    "hashtags_with_sentiments_lists.foreachRDD(add_to_df)\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    if not rdd.isEmpty():\n",
    "        try:\n",
    "            # Get spark sql singleton context from the current context\n",
    "            sql_context = get_sql_context_instance(rdd.context)\n",
    "            print(2)\n",
    "            # create a DF from RDD\n",
    "            hashtags_df = sql_context.createDataFrame(rdd).toDF(\"hashtag\", \"hashtag_count\")\n",
    "            print(3)\n",
    "            # Register the dataframe as table\n",
    "            hashtags_df.registerTempTable(\"hashtags\")\n",
    "            print(4)\n",
    "            # get the top 10 hashtags from the table using SQL and print them\n",
    "            hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "            print(5)\n",
    "            hashtag_counts_df.show()\n",
    "            print(type(hashtag_counts_df))\n",
    "            print(hashtag_counts_df.toPandas())\n",
    "            # call this method to prepare top 10 hashtags DF and send them\n",
    "            send_df_to_dashboard(hashtag_counts_df)\n",
    "            print(7)\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "        \n",
    "\n",
    "def send_df_to_dashboard(df):\n",
    "    # extract the hashtags from dataframe and convert them into array\n",
    "    top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "    # extract the counts from dataframe and convert them into array\n",
    "    tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "    # initialize and send the data through REST API\n",
    "    url = 'http://localhost:5001/updateData'\n",
    "    request_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "    response = requests.post(url, data=request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataStream = ssc.socketTextStream(twitter_credentials.HOST, twitter_credentials.PORT, StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "# adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "tags_totals.pprint()\n",
    "# do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplurning",
   "language": "python",
   "name": "deeplurning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
