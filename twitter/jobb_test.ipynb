{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for pyspark sql\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# imports for streaming\n",
    "from pyspark import SparkContext \n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# mllib imports \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "import time\n",
    "import pyspark\n",
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import re  # regex for cleaning the tweets\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "MY_SQL_CONNECTOR_PATH = \"mysql-connector-java-5.1.48.jar\"\n",
    "SPARK_LOG_CONF = 'true'\n",
    "DB_URL = \"127.0.0.1:3306/sql_hr\"\n",
    "DB_USER = \"root\"\n",
    "DB_PASSWORD = \"or211283\"\n",
    "DRIVER = \"com.mysql.jdbc.Driver\"\n",
    "BATCH_INTERVAL = 1 # base time unit (in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.logConf', SPARK_LOG_CONF)\n",
    "conf.set(\"spark.jars\", MY_SQL_CONNECTOR_PATH)\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"DecideForYou\").config(conf=conf).master(\"local[2]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second.Â \n",
    "ssc = StreamingContext(spark.sparkContext, BATCH_INTERVAL)\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplurning",
   "language": "python",
   "name": "deeplurning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
