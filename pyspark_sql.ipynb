{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext \n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "MY_SQL_CONNECTOR_PATH = \"mysql-connector-java-5.1.48.jar\"\n",
    "SPARK_LOG_CONF = 'true'\n",
    "DB_URL = \"127.0.0.1:3306/sql_hr\"\n",
    "DB_USER = \"root\"\n",
    "DB_PASSWORD = \"or211283\"\n",
    "DRIVER = \"com.mysql.jdbc.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.logConf', SPARK_LOG_CONF)\n",
    "conf.set(\"spark.jars\", MY_SQL_CONNECTOR_PATH)\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"DecideForYou\").config(conf=conf).master(\"local[2]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Create Spark Context\n",
    "#sc = SparkContext(\"local[2]\", \"spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wines = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "red_wines[\"is_red\"] = 1\n",
    "white_wines = pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "white_wines[\"is_red\"] = 0\n",
    "all_wines = pd.concat([red_wines, white_wines])\n",
    "all_wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = mysql.connector.connect(user=DB_USER, password=DB_PASSWORD)\n",
    "db_cursor = db_connection.cursor(buffered=True)\n",
    "\n",
    "DATABASE = \"sql_hr\"\n",
    "try:\n",
    "    db_cursor.execute(\"USE {};\".format(DATABASE))\n",
    "    \n",
    "    \n",
    "    db_cursor.execute(\"SELECT * FROM employees\")\n",
    "    employees = db_cursor.fetchall()\n",
    "    \n",
    "    employees_df = pd.DataFrame(employees, columns=['id', 'first name', 'last name', 'titel', 'salary','report to',\n",
    "                                                   'oice id'])\n",
    "    print(employees_df)\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Exception occured:{}\".format(e))\n",
    "\n",
    "finally:\n",
    "    db_cursor.close()\n",
    "    db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "USE sql_store;\n",
    "\n",
    "SET GLOBAL event_scheduler = OFF;\n",
    "\n",
    "\n",
    "# show events from schema\n",
    "Show events from sql_store;\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS testlock_event;\n",
    "# create testlock_event table\n",
    "CREATE TABLE testlock_event\n",
    "(\n",
    "\tid INT, \n",
    "\tcreated_at DATETIME NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS messages;\n",
    "CREATE TABLE messages \n",
    "(\n",
    "    id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    message VARCHAR(255) NOT NULL,\n",
    "    created_at DATETIME NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "DROP EVENT IF EXISTS test_event_01;\n",
    "\n",
    "UNLOCK TABLES ;\n",
    "\n",
    "CREATE EVENT IF NOT EXISTS test_event_01 # event name\n",
    "ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 MINUTE # event start time\n",
    "ON COMPLETION PRESERVE\n",
    "DO\n",
    "   INSERT INTO messages(message, created_at) # too what table and columns do insert\n",
    "   VALUES('Test MySQL Event 4',NOW()); # what values to insert\n",
    "\n",
    "\n",
    "DROP EVENT IF EXISTS test_event_02;\n",
    "CREATE EVENT IF NOT EXISTS test_event_02 # event name\n",
    "ON SCHEDULE EVERY 1 SECOND # event start time\n",
    "STARTS CURRENT_TIMESTAMP\n",
    "ENDS CURRENT_TIMESTAMP + INTERVAL 20 SECOND\n",
    "ON COMPLETION PRESERVE\n",
    "DO\n",
    "   INSERT INTO messages(message, created_at) # too what table and columns do insert\n",
    "   VALUES('Test MySQL Event 4',NOW()); # what values to insert\n",
    "\n",
    "\n",
    "SET @e = 0;\n",
    "\n",
    "DROP EVENT IF EXISTS testlock_event;\n",
    "DELIMITER |\n",
    "CREATE EVENT IF NOT EXISTS testlock_event ON SCHEDULE EVERY 2 SECOND DO\n",
    "BEGIN\n",
    " DECLARE CONTINUE HANDLER FOR SQLEXCEPTION # For SQLEXCEPTION conditions, the stored program terminates at the statement that raised the condition, as if there were an EXIT handler\n",
    " BEGIN\n",
    "   DO RELEASE_LOCK('testlock_event');\n",
    " END;\n",
    " DECLARE CONTINUE HANDLER FOR SQLWARNING\n",
    "  BEGIN\n",
    "     DO RELEASE_LOCK('testlock_event');\n",
    "  END;\n",
    " IF GET_LOCK('testlock_event', 0) THEN\n",
    "   -- add some business logic here, for example:\n",
    "\tinsert into testlock_event values(NULL, NOW());\n",
    "  END IF;\n",
    "  DO RELEASE_LOCK('testlock_event');\n",
    "END;\n",
    "|\n",
    "DELIMITER ;\n",
    "\n",
    "SELECT * FROM messages;\n",
    "\n",
    "SELECT @e AS 'Error number';\n",
    "\n",
    "\n",
    "# start scheduler process\n",
    "SET GLOBAL event_scheduler = ON;\n",
    "# show list of all running processes\n",
    "SHOW PROCESSLIST;\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = mysql.connector.connect(user=DB_USER, password=DB_PASSWORD)\n",
    "db_cursor = db_connection.cursor(buffered=True)\n",
    "# db_cursor.execute(\"CREATE DATABASE TestDB;\")\n",
    "# db_cursor.execute(\"USE TestDB;\")\n",
    "\n",
    "# db_cursor.execute(\"CREATE TABLE Wines(fixed_acidity FLOAT, volatile_acidity FLOAT, \\\n",
    "#                    citric_acid FLOAT, residual_sugar FLOAT, chlorides FLOAT, \\\n",
    "#                    free_so2 FLOAT, total_so2 FLOAT, density FLOAT, pH FLOAT, \\\n",
    "#                    sulphates FLOAT, alcohol FLOAT, quality INT, is_red INT);\")\n",
    "\n",
    "DATABASE = \"TestDB\"\n",
    "try:\n",
    "    db_cursor.execute(\"USE {};\".format(DATABASE))\n",
    "    \n",
    "        # start scheduler process\n",
    "    db_cursor.execute(\"SET GLOBAL event_scheduler = ON;\")\n",
    "    \n",
    "    # Execute the SQL statement \n",
    "    db_cursor.execute(\"DROP TABLE IF EXISTS messages;\")\n",
    "    \n",
    "    sql_create_table_massages = \"\"\" \n",
    "        CREATE TABLE messages \n",
    "        (\n",
    "        id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "        message VARCHAR(255) NOT NULL,\n",
    "        created_at DATETIME NOT NULL\n",
    "        );\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Execute the SQL statement \n",
    "    db_cursor.execute(sql_create_table_massages)\n",
    "\n",
    "    sqlCreateEvent = \"\"\"\n",
    "        CREATE EVENT IF NOT EXISTS test_event_01  \n",
    "        ON SCHEDULE AT CURRENT_TIMESTAMP + INTERVAL 1 MINUTE  \n",
    "        ON COMPLETION PRESERVE \n",
    "        DO  \n",
    "        INSERT INTO messages(message, created_at)  \n",
    "        VALUES('Test MySQL Event 4',NOW());\n",
    "    \"\"\"\n",
    "\n",
    "    db_cursor.execute(\"DROP EVENT IF EXISTS test_event_01\")\n",
    "    # Execute the SQL statement \n",
    "    db_cursor.execute(sqlCreateEvent)\n",
    "\n",
    "    sqlShowEvents = \"SHOW EVENTS FROM {};\".format(DATABASE)\n",
    "    db_cursor.execute(sqlShowEvents)\n",
    "\n",
    "    # Fetch all the rows\n",
    "    eventList = db_cursor.fetchall()\n",
    "    \n",
    "    print(\"List of events:\")\n",
    "    for event in eventList:\n",
    "        print(event)\n",
    "       \n",
    "    print(\"List of messages:\")\n",
    "    db_cursor.execute(\"SELECT * FROM messages;\")\n",
    "    messages = db_cursor.fetchall()\n",
    "    print(messages)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # start scheduler process\n",
    "    db_cursor.execute(\"SET GLOBAL event_scheduler = ON;\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Exception occured:{}\".format(e))\n",
    "\n",
    "finally:\n",
    "    db_cursor.close()\n",
    "    db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_tuples = list(all_wines.itertuples(index=False, name=None))\n",
    "wine_tuples_string = \",\".join([\"(\" + \",\".join([str(w) for w in wt]) + \")\" for wt in wine_tuples])\n",
    "\n",
    "db_cursor.execute(\"INSERT INTO Wines(fixed_acidity, volatile_acidity, citric_acid,\\\n",
    "                   residual_sugar, chlorides, free_so2, total_so2, density, pH,\\\n",
    "                   sulphates, alcohol, quality, is_red) VALUES \" + wine_tuples_string + \";\")\n",
    "db_cursor.execute(\"FLUSH TABLES;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(SELECT * FROM Wines) as Wines\"\n",
    "\n",
    "df = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/TestDB\") \\\n",
    "    .option(\"driver\", DRIVER) \\\n",
    "    .option(\"dbtable\", query) \\\n",
    "    .option(\"user\", DB_USER)\\\n",
    "    .option(\"password\", DB_PASSWORD) \\\n",
    "    .load() \\\n",
    "\n",
    "\n",
    "df.show()\n",
    "print(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|null|    1|\n",
      "|  19|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Working with a json file\n",
    "##########################\n",
    "\n",
    "\n",
    "# Read from json file \"people.json\"\n",
    "df = spark.read.json(\"people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema() \n",
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(\"name\", df['age'] + 1).show()\n",
    "\n",
    "# Select people older than 21\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "\n",
    "# Count people by age and sort by age\n",
    "df.groupBy(\"age\").count().sort('age').show()\n",
    "\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n",
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Working with a text file\n",
    "##########################\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = spark.sparkContext.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.show()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.write.csv('foo.csv', header=True)\n",
    "spark.read.csv('foo.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
